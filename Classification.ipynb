from numpy import arange
from numpy import argmax
from sklearn.metrics import f1_score

# define thresholds
thresholds = arange(0, 1, 0.001)

# apply threshold to positive probabilities to create labels
def to_labels(pos_probs, threshold):
	return (pos_probs >= threshold).astype('int')

# Now compile the model, train it on train sample and predict it for test sample.
def run_experiment(model):

    model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),
        loss=keras.losses.SparseCategoricalCrossentropy(),
        metrics=[keras.metrics.SparseCategoricalAccuracy()],
    )


    lst_roc_auc = []
    lst_precision = []
    lst_recall = []
    lst_f1 = []
    lst_acc = []
    lst_speci = []

    skf = StratifiedKFold(n_splits=10, shuffle=True)

    for i in range(10):
    #for i, (train_index, test_index) in enumerate(skf.split(X_data, Y_data)):
        all_data_train_fold, all_data_test_fold = all_data[train_index[i]], all_data[test_index[i]]
        #all_data_train_fold, all_data_test_fold = all_data[train_index], all_data[test_index]
        #print(train_index, test_index)

        # convert array into dataframe
        DF = pd.DataFrame(all_data_train_fold)
        # save the dataframe as a csv file
        DF.to_csv(train_data_file, index=False, header=False)
        train_data = get_dataset_from_csv(
           train_data_file, batch_size=batch_size, num_epochs=num_epochs
        )
        model.fit(train_data)

        # convert array into dataframe
        DF1 = pd.DataFrame(all_data_test_fold)
        # save the dataframe as a csv file
        DF1.to_csv(test_data_file, index=False, header=False)
        test_data = get_dataset_from_csv(
           test_data_file, batch_size=batch_size
        )
        #_, accuracy = model.evaluate(test_data)
        #lst_accu_stratified.append(round(accuracy, 2))

        #print(f"test_data: {test_data}")

        true_categories = tf.concat([y for x, y in test_data], axis=0)
        print(true_categories)

        # keep probabilities for the positive outcome only
        Y_pred = model.predict(test_data)
        probs = Y_pred[:, 1]
        #print(f"probs: {probs}")


        # evaluate each threshold
        #scores = [precision_score(true_categories, to_labels(probs, t)) for t in thresholds]
        #scores = [f1_score(true_categories, to_labels(probs, t)) for t in thresholds]
        scores = [roc_auc_score(true_categories, to_labels(probs, t)) for t in thresholds]
        # get best threshold
        ix = argmax(scores)
        #print(f"Optimal threshold score: {ix}")
        #print(f"Thresholds[ix]: {thresholds[ix]}")

        #Y_pred = model.predict(test_data)
        #probs = Y_pred[:, 1]
        #print(probs)
        #print(f"probsLast: {probs}")
        #print(f"to_labels(probs, thresholds[ix]): {to_labels(probs[len(probs)-1], thresholds[ix])}")

        conf_matrix = confusion_matrix(true_categories, to_labels(probs, thresholds[ix]))
        print(conf_matrix)

        tp, fn, fp, tn = conf_matrix.ravel()
        print(f"True positive: {tp}")
        print(f"True negative: {tn}")
        print(f"False positive: {fp}")
        print(f"False negative: {fn}")

        lst_roc_auc.append(roc_auc_score(true_categories,to_labels(probs, thresholds[ix])));
        lst_acc.append((tp+tn)/(tp+tn+fp+fn));

        if(tn+fp == 0):
           lst_speci.append(0)
        else:
           lst_speci.append(tn/(tn+fp));

        if(tp+fp == 0):
           precision = 0
           lst_precision.append(precision)
        else:
           precision = tp/(tp+fp)
           lst_precision.append(precision);

        if(tp+fn == 0):
           recall = 0
           lst_recall.append(recall)
        else:
           recall = tp/(tp+fn)
           lst_recall.append(recall);

        if(precision+recall == 0):
           lst_f1.append(0)
        else:
           lst_f1.append((2*precision*recall)/(precision+recall));

        print(f"roc_auc_score : {roc_auc_score(true_categories,to_labels(probs, thresholds[ix]))}")
        print(f"precision : {precision}")
        print(f"recall : {recall}")
        print(f"accuracy : {(tp+tn)/(tp+tn+fp+fn)}")
        print(f"f1_score : {(2*precision*recall)/(precision+recall)}")
        print(f"specificity : {tn/(tn+fp)}")

        print('')

    print(f"mean_of_accuracy : {mean(lst_acc)}")
    print(f"mean_of_roc_auc_score : {mean(lst_roc_auc)}")
    print(f"mean_of_precision : {mean(lst_precision)}")
    print(f"mean_of_recall : {mean(lst_recall)}")
    print(f"mean_of_f1_score : {mean(lst_f1)}")
    print(f"mean_of_specificity : {mean(lst_speci)}")